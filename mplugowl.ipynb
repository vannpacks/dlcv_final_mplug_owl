{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataset.star import STAR\n",
    "# data loader\n",
    "from torch.utils.data import DataLoader\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import json\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/vann/anaconda3/envs/mplug_owl/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "install flash-attn first.\n",
      "Model loaded.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from mplug_owl_video.modeling_mplug_owl import MplugOwlForConditionalGeneration\n",
    "from transformers import AutoTokenizer\n",
    "from mplug_owl_video.processing_mplug_owl import MplugOwlImageProcessor, MplugOwlProcessor\n",
    "\n",
    "pretrained_ckpt = 'MAGAer13/mplug-owl-llama-7b-video'\n",
    "# model = MplugOwlForConditionalGeneration.from_pretrained(\n",
    "#     pretrained_ckpt,\n",
    "#     torch_dtype=torch.float16,\n",
    "# )\n",
    "model = MplugOwlForConditionalGeneration.from_pretrained( pretrained_ckpt, device_map={'': 0}, torch_dtype=torch.bfloat16)\n",
    "\n",
    "image_processor = MplugOwlImageProcessor.from_pretrained(pretrained_ckpt)\n",
    "tokenizer = AutoTokenizer.from_pretrained(pretrained_ckpt)\n",
    "processor = MplugOwlProcessor(image_processor, tokenizer)\n",
    "\n",
    "print('Model loaded.')\n",
    "# We use a human/AI template to organize the context as a multi-turn conversation.\n",
    "# <|video|> denotes an video placehold.\n",
    "# prompts = [\n",
    "# '''The following is a conversation between a curious human and AI assistant. The assistant gives helpful, detailed, and polite answers to the user's questions.\n",
    "# Human: <|video|>\n",
    "# Human: Which object was put down by the person? 0. \"The food., 1. \"The laptop.\", 2. \"The book.\", 3. \"The pillow.\"\n",
    "# AI: ''']\n",
    "\n",
    "\n",
    "# video_list = ['YSKX3.mp4']\n",
    "\n",
    "# # generate kwargs (the same in transformers) can be passed in the do_generate()\n",
    "# generate_kwargs = {\n",
    "#     'do_sample': True,\n",
    "#     'top_k': 5,\n",
    "#     'max_length': 512\n",
    "# }\n",
    "# print('Start generating...')\n",
    "# inputs = processor(text=prompts, videos=video_list, num_frames=4, return_tensors='pt')\n",
    "# # inputs = {k: v.bfloat16() if v.dtype == torch.float else v for k, v in inputs.items()}\n",
    "# inputs = {k: v.to(model.device) for k, v in inputs.items()}\n",
    "# with torch.no_grad():\n",
    "#     res = model.generate(**inputs, **generate_kwargs)\n",
    "# sentence = tokenizer.decode(res.tolist()[0], skip_special_tokens=True)\n",
    "# print(sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read csv into question_id,video_id,Keyframe_IDs\n",
    "import csv\n",
    "import ast\n",
    "qid_list = []\n",
    "vid_list = []\n",
    "kfid_list = []\n",
    "# Open the CSV file\n",
    "with open('Video_Keyframe_IDs.csv', 'r') as csvfile:\n",
    "    # Create a CSV reader object\n",
    "    csvreader = csv.DictReader(csvfile)\n",
    "\n",
    "    # Iterate through each row in the CSV file\n",
    "    for row in csvreader:\n",
    "        # row variable is a dictionary that represents a row in csv\n",
    "        qid_list.append(row['question_id'])\n",
    "        vid_list.append(row['video_id'])\n",
    "        kfid_list.append(row['Keyframe_IDs'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### testing with single video\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We use a human/AI template to organize the context as a multi-turn conversation.\n",
    "# <|video|> denotes an video placehold.\n",
    "# prompts = [\n",
    "# '''\n",
    "# Which object was put down by the person? 0. \"The food., 1. \"The laptop.\", 2. \"The book.\", 3. \"The pillow.\".\n",
    "# The answer is ''',\n",
    "\n",
    "# '''\n",
    "# Which object was put down by the person? 0. \"The clothes., 1. \"The pillow.\", 2. \"The towel.\", 3. \"The laptop.\".\n",
    "# The answer is ''',\n",
    "# ]\n",
    "\n",
    "prompts = [\n",
    "'''\n",
    "Human: <|video|>\n",
    "Human: What is this video about?\n",
    "AI: ''',\n",
    "]\n",
    "# '''\n",
    "# Human: <|video|>\n",
    "# Human: What is this video about?\n",
    "# AI: '''\n",
    "\n",
    "# ]\n",
    "\n",
    "video_list = [\"videos/\" + vid_list[0]+'.mp4']\n",
    "key_frame_list = [ast.literal_eval(kfid_list[0])]\n",
    "# video_list = ['YSKX3.mp4', 'E9UYZ.mp4']\n",
    "\n",
    "# generate kwargs (the same in transformers) can be passed in the do_generate()\n",
    "generate_kwargs = {\n",
    "    'do_sample': False,\n",
    "    'top_k': 3,\n",
    "    'max_length': 300\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The video shows a young man, possibly a boy, playing with a bed sheet on a bed in a room. He is jumping and having fun, creating a playful atmosphere. The room is well-lit and has a few chairs and a table nearby. The bed is situated in the center of the room, with the bed sheet hanging from the bed frame.\n"
     ]
    }
   ],
   "source": [
    "inputs = processor(text=prompts, videos=video_list, kfid = key_frame_list,num_frames=10, return_tensors='pt')\n",
    "inputs = {k: v.bfloat16() if v.dtype == torch.float else v for k, v in inputs.items()}\n",
    "inputs = {k: v.to(model.device) for k, v in inputs.items()}\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    res = model.generate(**inputs, **generate_kwargs)\n",
    "for i in res.tolist():\n",
    "    sentence = tokenizer.decode(i, skip_special_tokens=True)\n",
    "    print(sentence)\n",
    "    \n",
    "# sentence = tokenizer.decode(res.tolist()[0], skip_special_tokens=True)\n",
    "# print(sentence)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### iterate all test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num test data: 7377\n"
     ]
    }
   ],
   "source": [
    "test_data = STAR(\"test\")\n",
    "data_lo1der = DataLoader(test_data, batch_size=4, shuffle=False, num_workers=3)\n",
    "\n",
    "generate_kwargs = {\n",
    "    'do_sample': False,\n",
    "    'top_k': 3,\n",
    "    'max_length': 8\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The following is a conversation between a human and AI assistant. The assistant gives accurate answers to the user's question.\n",
      "Human: <|video|>\n",
      "Human: Which object was put down by the person?, \n",
      "AI: The answer is \n",
      "AI: a bed sheet.\n",
      "The following is a conversation between a human and AI assistant. The assistant gives accurate answers to the user's question.\n",
      "Human: <|video|>\n",
      "Human: Which object was taken by the person?, \n",
      "AI: The answer is \n",
      "AI: a pillow.\n",
      "The following is a conversation between a human and AI assistant. The assistant gives accurate answers to the user's question.\n",
      "Human: <|video|>\n",
      "Human: Which object was thrown by the person?, \n",
      "AI: The answer is \n",
      "AI: a pillow.\n",
      "The following is a conversation between a human and AI assistant. The assistant gives accurate answers to the user's question.\n",
      "Human: <|video|>\n",
      "Human: Which object was tidied up by the person?, \n",
      "AI: The answer is \n",
      "AI: a bed.\n",
      "The following is a conversation between a human and AI assistant. The assistant gives accurate answers to the user's question.\n",
      "Human: <|video|>\n",
      "Human: Which object was taken by the person?, \n",
      "AI: The answer is \n",
      "AI: a towel.\n",
      "The following is a conversation between a human and AI assistant. The assistant gives accurate answers to the user's question.\n",
      "Human: <|video|>\n",
      "Human: Which object was tidied up by the person?, \n",
      "AI: The answer is \n",
      "AI: a towel.\n",
      "The following is a conversation between a human and AI assistant. The assistant gives accurate answers to the user's question.\n",
      "Human: <|video|>\n",
      "Human: Which object was opened by the person?, \n",
      "AI: The answer is \n",
      "AI: a laptop.\n",
      "The following is a conversation between a human and AI assistant. The assistant gives accurate answers to the user's question.\n",
      "Human: <|video|>\n",
      "Human: Which object was thrown by the person?, \n",
      "AI: The answer is \n",
      "AI: a red scarf.\n",
      "The following is a conversation between a human and AI assistant. The assistant gives accurate answers to the user's question.\n",
      "Human: <|video|>\n",
      "Human: Which object was put down by the person?, \n",
      "AI: The answer is \n",
      "AI: a cell phone.\n",
      "The following is a conversation between a human and AI assistant. The assistant gives accurate answers to the user's question.\n",
      "Human: <|video|>\n",
      "Human: Which object was sat on by the person?, \n",
      "AI: The answer is \n",
      "AI: a chair.\n",
      "The following is a conversation between a human and AI assistant. The assistant gives accurate answers to the user's question.\n",
      "Human: <|video|>\n",
      "Human: Which object was put down by the person?, \n",
      "AI: The answer is \n",
      "AI: a cup.\n",
      "The following is a conversation between a human and AI assistant. The assistant gives accurate answers to the user's question.\n",
      "Human: <|video|>\n",
      "Human: Which object was tidied up by the person?, \n",
      "AI: The answer is \n",
      "AI: a dish.\n",
      "The following is a conversation between a human and AI assistant. The assistant gives accurate answers to the user's question.\n",
      "Human: <|video|>\n",
      "Human: Which object was opened by the person?, \n",
      "AI: The answer is \n",
      "AI: a bag.\n",
      "The following is a conversation between a human and AI assistant. The assistant gives accurate answers to the user's question.\n",
      "Human: <|video|>\n",
      "Human: Which object was lied on by the person?, \n",
      "AI: The answer is \n",
      "AI: a chair.\n",
      "The following is a conversation between a human and AI assistant. The assistant gives accurate answers to the user's question.\n",
      "Human: <|video|>\n",
      "Human: Which object was eaten by the person?, \n",
      "AI: The answer is \n",
      "AI: a piece of cake.\n",
      "The following is a conversation between a human and AI assistant. The assistant gives accurate answers to the user's question.\n",
      "Human: <|video|>\n",
      "Human: Which object was put down by the person?, \n",
      "AI: The answer is \n",
      "AI: a spoon.\n",
      "The following is a conversation between a human and AI assistant. The assistant gives accurate answers to the user's question.\n",
      "Human: <|video|>\n",
      "Human: Which object was tidied up by the person?, \n",
      "AI: The answer is \n",
      "AI: a coffee maker.\n",
      "The following is a conversation between a human and AI assistant. The assistant gives accurate answers to the user's question.\n",
      "Human: <|video|>\n",
      "Human: Which object was taken by the person?, \n",
      "AI: The answer is \n",
      "AI: a bottle of water.\n",
      "The following is a conversation between a human and AI assistant. The assistant gives accurate answers to the user's question.\n",
      "Human: <|video|>\n",
      "Human: Which object was opened by the person?, \n",
      "AI: The answer is \n",
      "AI: a refrigerator.\n",
      "The following is a conversation between a human and AI assistant. The assistant gives accurate answers to the user's question.\n",
      "Human: <|video|>\n",
      "Human: Which object was closed by the person?, \n",
      "AI: The answer is \n",
      "AI: the refrigerator door.\n",
      "The following is a conversation between a human and AI assistant. The assistant gives accurate answers to the user's question.\n",
      "Human: <|video|>\n",
      "Human: Which object was put down by the person?, \n",
      "AI: The answer is \n",
      "AI: a plate.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "model.eval()\n",
    "answers =[]\n",
    "# for id, data in tqdm(enumerate(test_data), total=len(test_data)):\n",
    "for id, data in enumerate(test_data):\n",
    "    if id > 20:\n",
    "        break\n",
    "    text, qid, answer = data\n",
    "    qid_index = qid_list.index(qid)\n",
    "    video_list = [\"videos/\" + vid_list[qid_index]+'.mp4']\n",
    "    key_frame_list = [ast.literal_eval(kfid_list[qid_index])]\n",
    "    question = text['q_text'].split(':')[1].split(\"\\n\")[0]\n",
    "    #remove first space\n",
    "    question = question[1:]\n",
    "    option = text['o_text'].split(':')[1]\n",
    "    choices = text['options']\n",
    "    ans = text['a_text'].split(':')[1]\n",
    "    \n",
    "#     prompts = [\n",
    "#     f'''The following is a conversation between a human and AI assistant. The assistant gives an accurate answer to the user's question.\n",
    "# Human: <|video|>\n",
    "# Human: {question},  0. {choices[0]}, 1. {choices[1]}, 2. {choices[2]}, 3. {choices[3]}, choose between these options.\n",
    "# AI: The answer is ''']\n",
    "\n",
    "    prompts = [\n",
    "    f'''The following is a conversation between a human and AI assistant. The assistant gives accurate answers to the user's question.\n",
    "Human: <|video|>\n",
    "Human: {question}, \n",
    "AI: The answer is '''\n",
    "]\n",
    "\n",
    "\n",
    "\n",
    "    \n",
    "    inputs = processor(text=prompts, videos=video_list, kfid = key_frame_list,num_frames=12, return_tensors='pt')\n",
    "    \n",
    "    inputs = {k: v.bfloat16() if v.dtype == torch.float else v for k, v in inputs.items()}\n",
    "    inputs = {k: v.to(model.device) for k, v in inputs.items()}\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        res = model.generate(**inputs, **generate_kwargs)\n",
    "    print(prompts[0])\n",
    "    for k in res.tolist():\n",
    "        sentence = tokenizer.decode(k, skip_special_tokens=True)\n",
    "        answers.append(sentence)\n",
    "        print(\"AI: \" + sentence)\n",
    "\n",
    "with open('answers.json', 'w') as f:\n",
    "    json.dump(answers, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### generate output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "answer_options = [int(i[0]) for i in answers]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[3, 3, 3, 2, 2, 3, 2, 2, 2, 2, 3, 2, 2, 2, 2, 0, 3, 3, 2, 0, 2]"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "answer_options"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "answer_qid_list = []\n",
    "for id, data in tqdm(enumerate(test_data), total=len(test_data)):\n",
    "    text, qid, answer = data\n",
    "    answer_qid_list.append(qid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_answers_dict = {\"Interaction\":[], \"Sequence\":[], \"Prediction\" : [], \"Feasibility\": []}\n",
    "for id, answer_qid in enumerate(answer_qid_list):\n",
    "    if \"Interaction\" in answer_qid:\n",
    "        final_answers_dict[\"Interaction\"].append({\"question_id\":answer_qid, \"answer\":int(answer_options[id])})\n",
    "    elif \"Sequence\" in answer_qid:\n",
    "        final_answers_dict[\"Sequence\"].append({\"question_id\":answer_qid, \"answer\":int(answer_options[id])})\n",
    "    elif \"Prediction\" in answer_qid:\n",
    "        final_answers_dict[\"Prediction\"].append({\"question_id\":answer_qid, \"answer\":int(answer_options[id])})\n",
    "    elif \"Feasibility\" in answer_qid:\n",
    "        final_answers_dict[\"Feasibility\"].append({\"question_id\":answer_qid, \"answer\":int(answer_options[id])})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [],
   "source": [
    "#dump to json\n",
    "with open('final_answers.json', 'w') as f:\n",
    "    json.dump(final_answers_dict, f)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mplug_owl",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
